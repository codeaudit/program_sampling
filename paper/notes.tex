\documentclass{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{verbatim}
\begin{document}

\section{High-level overview}

We want to sample programs from a description length prior.
Let $x$ be a program. Sample from
\begin{equation}
  P(x) = \frac{2^{-|x|}}{Z}
\end{equation}
Assume $|x|\leq n$ always, where $n$ is the number of SAT variables encoding the structure of the program.

The approach will draw from two techniques: (1) constraint-solving techniques for program synthesis; and (2) uniform solution sampling using XOR constraints.

The two most obvious approaches to this problem don't work in practice:

\begin{itemize}
\item{Prefix codes}: Write down a prefix code for the space of allowed programs.
  Uniformly sample values of all $n$ SAT variables encoding the program structure;
  then the probability of sampling $x$ is proportional to $2^{n-|x|}$.
\item{Embed \& Project}: Introduce $n$ auxiliary variables, $A$,
  and add constraints $|x| > j\implies A_{j+1}$.
  Uniformly sample solutions to $(x,A)$.
  This is the technique introduced in ``Embed and Project'' (Stefano Ermon, NIPS 2013)
\end{itemize}

The problem with these approaches is that
they dramatically increase the number of satisfyable solutions to the underlying SAT formula by a factor of $O(2^n)$,
and so require at least about $n$ random constraints;
for large $n$, the solver scales exponentially with the number of random constraints (empirically observed for even CryptoMiniSAT).
The body of work on sampling with random constraints has previously restricted itself to cases where the number of satisfiable solutions is relatively small ($ < 600$ in the original Gomes et al. NIPS paper; on the order of tens of thousands in later work, by the authors of UniGen).

Our idea is to use auxiliary variables as in ``Embed and Project,'' but reduce the dimensionality of the embedding
by only hashing the first $\alpha$ bits of $A$.
If we put $\alpha = n$ then our samples come from $P$; otherwise they come from a different distribution $Q$:
\begin{equation}
  Q(x) \propto 2^{-\min (\alpha,|x| )}
  \end{equation}
We can correct for this proposal distribution by accepting a sample $x$ from $Q$ with probability $A(|x|,\alpha) = 2^{\min(0,\alpha - |x|)}$.
So we accept any sample whose description length is bounded by $\alpha$ and accept longer samples with probability $2^{\alpha - |x|}$.
This satisfies $Q(x)A(|x|,\alpha)\propto P(x)$.

The only reason that this works in the general case is because of the assumption that $|x|\leq n$,
which allows us to make $Q$ arbitrarily close to $P$ (and thus arbitrarily decrease the rejection rate),
but at the expense of needing more random constraints.
The rest of this document analyzes the trade-off between increasing $\alpha$ and increasing $K$ (the number of random constraints),
and then presents some experimental results.

\section{Algorithmic details}
\subsection{The algorithm}
Let $X$ be a set of programs satisfying some specification, such as agreeing with certain input-output pairs.
We want to sample from
\begin{equation}
  P(x\in X) = \frac{2^{-|x|}}{Z}
\end{equation}
where $Z = \sum_{x\in X}2^{-|x|}$.

Let $\alpha$ be the dimensionality of the embedding.
The set $E$ of programs in the embedded space is
\begin{equation}
  E = \{(x,A) : x\in X, |x|> j \implies A_{j+1}\text{ when } j<\alpha \}
\end{equation}

In each iteration of the algorithm we sample a random projection from $n+\alpha$ to $K$ variables, setting it equal to a random $K$ dimensional vector.
Let  $h(x,A)$ be true if $(x,A)\in E$ satisfies this random constraint.
Let $S = \{ e\in E : h(e)\}$.
If $S$ is empty, reject this attempt at sampling.
Otherwise draw uniformly from $S$ to obtain $(x,A)$,
and reject this attempt at sampling with probability $1 - A(|x|,\alpha) = 1 - 2^{\min(0,\alpha - |x|)}$.

The embedding introduces many symmetries (lots of $e$'s that have the same $x$),
and we can break this symmetry by only enumerating unique values of $x$.
For a given $x$, the number of $(x,A)\in S$ can be calculated without further calls to the solver:
it is $2^{\alpha - |x| - \text{rank}(H_{>n+|x|})}$ where $H$ is the matrix used for the random hash.

\subsection{Algorithm analysis}
First, what is the probability that a sample from $Q$ is accepted?
Call this $P(\text{accepted})$:
\begin{eqnarray}
  P(\text{accepted}) &=& \sum_x Q(x)A(|x|,\alpha)\\
  & = &\sum_x \frac{2^{-|x|}}{\sum_{x'}Q(x')}\\
  & = &\frac{Z}{\sum_{|x|>\alpha}2^{-\alpha} + \sum_{|x|\leq\alpha}2^{-|x|}}\\
  & = &\frac{Z}{Z - Z_\alpha + 2^{-\alpha}C_\alpha}\\
  & = &\frac{1}{1 + 2^{-\alpha}C_\alpha/Z - P(|x|>\alpha)}
\end{eqnarray}
where $C_\alpha = |\{x\in X : |x|>\alpha\}|$ and $Z_\alpha = \sum_{|x|>\alpha}2^{-|x|} $.

Second, what is the probability that we get a sample from $Q$?
Let $mc$ be the model count (members of $E$ that survived the hashing constraint).
Then $E[mc] = \mu = 2^{-K}|E|$ and $\text{Variance}[mc] = \sigma^2 = |E|2^{-K}(1 - 2^{-K})$.
We fail to get a sample if $mc<1$, equivalently, $mc\leq \epsilon$ for $0<\epsilon < \min (\mu,1)$.
\begin{eqnarray}
  P(mc\leq \epsilon)&<& P(|mc-\mu |\geq \mu -  \epsilon)\\
  &\leq&\frac{\sigma^2}{(\mu - \epsilon)^2}\text{, Chebyshev 's inequality}\\
  &\leq&\frac{\mu}{(\mu - \epsilon)^2}.
\end{eqnarray}

Put these together to bound the probability of failing to get a sample:
\begin{eqnarray}
  P(\text{fail to get sample})& = &P(mc = 0) + P(mc > 0)(1 - P(\text{accepted}))\\
  &<&\frac{\mu}{(\mu - \epsilon)^2} + 1 - \frac{1}{1 + 2^{-\alpha}C_\alpha/Z - P(|x|>\alpha)}\\
  P(\text{get sample})&>& \frac{1}{1 + 2^{-\alpha}C_\alpha/Z - P(|x|>\alpha)} - \frac{\mu}{(\mu - \epsilon)^2}\\
\end{eqnarray}

Now that we have a bound on the probability of getting a sample (and thus abound on the expected number of attempts needed to get a sample),
we want to analyze how much work we expect to do on each attempt at sampling.
A reasonable proxy for this is the number of invocations made to the solver,
which is just the number of programs in $E$ that survived the constraints (\emph{not} the number of surviving solutions).
Let $x$ be a program.
If $|x|>\alpha$ then the probability that it survives is just $2^{-K}$.
Otherwise it's more complicated.
Let $|x|\leq \alpha$, $H$ be the random matrix and $b$ the random vector.
The hashing constraint reads
\begin{equation}
  b_i = \sum_{j = 1}^n H_{ij}x_j + \sum_{j = n+1}^{n+|x|}H_{ij}A_{j-n} + \sum_{j = n + |x| + 1}^{n + \alpha}H_{ij}A_{j-n}
\end{equation}
The first two sums can be folded into $b_i$, giving
\begin{equation}
  b'_i = \sum_{j = n + |x| + 1}^{n + \alpha}H_{ij}A_{j-n}
\end{equation}
So $x$ survives if $b'\in\text{range}(H_{j\geq n + |x| + 1})$.
The matrix $H_{j\geq n + |x| + 1}$ is $K$ by $\alpha - |x|$; call it $G$.
\begin{eqnarray}
  P(x \text{ survives})& = &\sum_{r = 0}^{\min(K,\alpha - |x|)}P(b'\in\text{range}(G) | \text{rank}(G) = r)P(\text{rank}(G) = r)\\
  & = &\sum_r 2^{r - K}2^{-K(\alpha - |x|)}\prod_{0\leq i<r}\frac{(2^K - 2^i)(2^{\alpha - |x|} - 2^i)}{2^r - 2^i}\\
  &<&\min (1,2^{\alpha - |x| - K})
\end{eqnarray}
where the last inequality is not derived from the former inequality but from applying the union bound to the original statement.

What is the total expected work to get one sample?
Let $s_i(x)$ be an indicator variable for whether $x$ survives the $i$th sampling attempt,
and $f_i$ be an indicator variable for whether the $i$th sampling attempt failed.
the number of solver invocations is
\begin{eqnarray*}
  &&1 + \sum_x s_1(x)\\
  &+&f_1 + \sum_xs_2(x)f_1\\
  &+&f_1f_2 + \sum_xs_3(x)f_1f_2\\
  &+&\cdots
\end{eqnarray*}
which has the expectation
\begin{equation}
  \frac{1 + \sum_x P(x \text{ survives})}{P(\text{get sample})}
\end{equation}

We can upper bound this quantity using the previously bounded quantities.
The only approximation comes in here:
\begin{equation}
  p(\text{no survivors}) < \frac{\mu}{(\mu - \epsilon)^2}
\end{equation}
Thus by enumerating all of the solutions to a program synthesis problem we can compute fairly tight bounds on the performance of the proposed algorithm,
as the next section details.

In practice we want to sample without enumerating all of the solutions.
Notice that $\mu = |E|2^{-K} > (|S| - 1 + 2^{\alpha - L})2^{-K}$ where $L = \min_x |x|$.
So if $K$ is much less than $\log (|S| - 1 + 2^{\alpha - L})$ then $1/\mu$ is close to zero.
More precisely,
introduce a parameter $\delta > 0$,
\begin{eqnarray}
  K + \delta& = &\log (|S| - 1 + 2^{\alpha - L})\\
  2^\delta& = &2^{-K}  (|S| - 1 + 2^{\alpha - L}) < \mu\\
  2^{-\delta}& > &1/\mu
\end{eqnarray}
From here on out assume that $K + \delta = \log (|S| - 1 + 2^{\alpha - L})$ (look at the experimental results: you only get good sampling when you are in a region that falls below a diagonal line on the heat maps. This diagonal line corresponds to this constraint)
and plug the above result into the upper bound:
\begin{eqnarray}
  E[\text{invocations}]& =&   \frac{1 + \sum_x P(x \text{ survives})}{P(\text{get sample})}\\
  &<& \frac{1 + \sum_x P(x \text{ survives})}{\frac{1}{1 + 2^{-\alpha}C_\alpha/Z - P(|x|>\alpha)} - 2^{-\delta}}
\end{eqnarray}

For large enough $\alpha$, the probability of a sample from $Q$ being accepted has a simple bound.
Assume from here on out that $\alpha = \log |S| + L + \gamma$ for $\gamma > 0$:
\begin{eqnarray}
  1/P(\text{accepted}) &= &2^{-\alpha}C_\alpha/Z + P(|x|\leq \alpha)\\
  &  < &2^{-\alpha}|S|2^L + 1\\
  & = &2^{-\log |S|-L - \gamma}|S|2^L + 1\\
  & = &1 + 2^{ - \gamma}
\end{eqnarray}
So the probability of acceptance approaches 1 exponentially quickly once $\alpha$ gets big enough.
Looking at the heat maps in the experimental results,
we see that we don't get good samplers until $\alpha - L$ crosses some threshold that increases as the number of possible programs increases.
This qualitative feature of the heat maps is reflected in the above analysis.

The last piece of the approximation is to show that $K$ cannot be too small compared to alpha or else the number of programs enumerated in each round of sampling grows two large.
\begin{comment}
\begin{eqnarray}
  \sum_x P(x\text{ survives})& = &C_\alpha 2^{-K} + \sum_{|x|\leq \alpha}\min (1,2^{\alpha - |x| - K})\\
  & < &C_\alpha 2^{-K} + \sum_{|x|\leq \alpha}\min (1,2^{\alpha - L - K})\\
   &= &C_\alpha 2^{-K} + \sum_{|x|\leq \alpha}\min (1,2^{\gamma + \log |S| - K})\\
  &= &C_\alpha 2^{-K} + \sum_{|x|\leq \alpha}\min (1,|S| 2^{\gamma - K})
\end{eqnarray}
if $K > \alpha - L$:
\begin{eqnarray}
  & < &C_\alpha 2^{-K} + \sum_{|x|\leq \alpha}|S| 2^{\gamma - K}  
\end{eqnarray}
but that bound is terrible and takes us into the white region of the heat maps - don't take this route! If $K < \alpha - L$:
\begin{eqnarray}
  & < &C_\alpha 2^{-K} + \sum_{|x|\leq \alpha}1 < |S|\text{, equally useless}
\end{eqnarray}
Trying something else:
\begin{eqnarray}
  \sum_x P(x\text{ survives})& = &C_\alpha 2^{-K} + \sum_{|x|\leq \alpha}\min (1,2^{\alpha - |x| - K})\\
  & < &C_\alpha 2^{-K} + \sum_{|x|\leq \alpha} 2^{\alpha - |x|- K}\\
   &= &2^{-K}(C_\alpha + \sum_{|x|\leq \alpha}2^{\alpha - |x|}) = 2^{-K}|E|\\
& < &2^{-K}|S|2^{\alpha - L}
\end{eqnarray}
Equally useless. Take a different approach: the top is decreasing in $K$ and the bottom is increasing in $K$.
What can this tell us about the optimal value of $K$?
$$
E[tt] < \frac{1 + \sum_x P(x\text{ survives})}{\frac{1}{1 + 2^{ - \gamma}} - 2^{ - \delta}} = \frac{1 + 2^{ - \gamma}}{1 - 2^{ - \delta} - 2^{ - \delta - \gamma}}(1 + \sum_x P(x\text{ survives}))
$$
$\alpha$ is fixed by parameters of the problem and $\gamma$. In contrast $K$ decreases linearly with $\delta$.
\begin{eqnarray}
  \partial_K \log E[tt] &<& \frac{\partial_K (2^{-\delta}+2^{ - \delta - \gamma})}{1 - 2^{ - \delta} - 2^{ - \delta - \gamma}}+...\\
  & = &   \frac{\log 2 \times (2^{-\delta} +2^{ - \delta - \gamma})}{1 - 2^{ - \delta} - 2^{ - \delta - \gamma}}+
\frac{-C_\alpha 2^{-K}\log 2 }{1 + \sum_x P(x\text{ survives})}
  \\
  \end{eqnarray}
Unfinished. And not looking promising. Return to the horrible bounds, which grossly overestimate $|E|$ and fail to take into account the smarter enumeration strategy:
\begin{align}
  E[tt] <& \frac{2^{-K}|S|2^{\alpha - L}(1 - 2^{ - \gamma})}{1 - 2^{ - \delta} - 2^{ - \gamma - \delta}}\\
  = &\frac{2^{\delta - \log (|S| - 1 + 2^{\alpha - L})}|S|2^{\alpha - L}(1 - 2^{ - \gamma})}{1 - 2^{ - \delta} - 2^{ - \gamma - \delta}}\\
  = &\frac{2^\delta\frac{|S|2^{\alpha - L}}{|S| - 1 + 2^{\alpha - L}}}{1 - 2^{ - \delta} - 2^{ - \gamma - \delta}}(1 - 2^{ - \gamma})\\
  \propto &\frac{2^\delta}{1 - 2^{ - \delta}(1 + 2^{ - \gamma})}\text{, as a function of $\delta$}
\end{align}
Minimize the above found subject to $\delta > 0$:
\begin{align}
  &\partial_\delta \ln\frac{2^\delta}{1 - 2^{ - \delta}(1 + 2^{ - \gamma})}\\
  = &\ln 2  (2^\delta - 2(1 + 2^{ - \gamma}))/(2^\delta - (1 + 2^{ - \gamma})) =  0\\
  2^\delta =& 2(1 + 2^{ - \gamma})\\
  \delta = &1 + \log_2 (1 + 2^{ - \gamma})
\end{align}
Substitute this into the really bad bound on the number of solver calls:
\begin{align}
  E[tt] < & (1 + \log_2 (1 + 2^{ - \gamma})) \frac{|S|2^{\alpha - L}}{|S| - 1 + 2^{\alpha - L}}(1 - 2^{ - \gamma})2\\
  = &2(1 + \log_2 (1 + 2^{ - \gamma})) \frac{|S|^2 2^\gamma}{|S| - 1 + |S|2^\gamma}(1 - 2^{ - \gamma})\\
  = &2(1 + \log_2 (1 + 1/x)) (1 - 1/x) \frac{|S|^2 x}{|S| - 1 + |S|x}\text{, }x = 2^\gamma\\
   = &2(1 + \log_2 (1 + 1/x)) (1 - 1/x)\frac{|S|x}{x + 1 - 1/|S|}
\end{align}
The first part goes to 1 and the second part goes to $|S|$, both exponentially quickly in $\gamma$. This bound is no better than just enumerating all of the solutions!
\end{comment}

%\subsubsection{Another attack on the upper bound}
\begin{align}
\sum_x P(s_x) & = \sum_{\alpha - K\leq |x|}\max (2^{-K},2^{\alpha -K - |x|}) + C_{ < \alpha - K}
\end{align}
Suppose that we hold $\alpha - K$ constant, so that we are restricting ourselves to a diagonal line on the heat maps.
Then the above equation is decreasing in $K$, so as we walk down  the diagonal we get increasingly better samplers (and walking all the way down corresponds to prior work).
What if we don't restrict ourselves to a diagonal?
Consider the case of moving $K$ while keeping $\alpha$ constant.
Number of survivors grows like $2^{-K}\propto 2^\delta$,
number of iterations grows like $1/(1 - 2^{ - \delta}(1 + 2^{ - \gamma}))$.
let $w = 2^\delta$, then our loss function looks like $w^2/(w - (1 + 2^{ - \gamma}))$,
which achieves its minimum at $\delta = 1 +\log (1 + 2^{ - \gamma})$.
So for large $\gamma$ we expect the optimal $\delta$ to be close to $1$.
Looking at the heat maps it seems that a better value is $2$, but that $1$ is pretty close.


Let's simplify things a little bit:
\begin{align}
  \alpha& = \log |S| +L + \gamma\\
  K &= \alpha - L - \delta\\
  & = \log |S| + \gamma - \delta
\end{align}
With these new definitions of the parameters,
\begin{align}
  E[tt] =& \frac{1 + \sum_{\alpha - K\leq |x|}\max (2^{-K},2^{\alpha -K - |x|}) + C_{ < \alpha - K}}{1 - 2^{ - \delta}(1 + 2^{ - \gamma})}(1 + 2^{ - \gamma})\\
   =& \frac{1 + 2^\delta\sum_{|x|\geq L + \delta}\max (2^{ - \gamma}/|S|,2^{L  - |x|}) + C_{ < L + \delta}}{1 - 2^{ - \delta}(1 + 2^{ - \gamma})}(1 + 2^{ - \gamma})\\
\end{align}
which is much clearer.

\subsection{KL divergence of the proposal distribution}
\begin{align}
  D(P||Q)& = \sum_x P(x)\log P(x)/Q(x)\\
  & = \sum_{|x|\leq \alpha} P(x)\log \frac{2^{-|x|}Z_Q}{2^{-|x|}Z} + \sum_{|x| > \alpha}P(x)\log \frac{2^{-|x|}Z_Q}{2^{-\alpha}Z}\\
  & = \log (Z_Q/Z) + \sum_{|x| > \alpha} P(x)(\alpha - |x|)\\
  & < \log (Z_Q/Z)\\
  & = \log \frac{Z - Z_\alpha + C_\alpha2^{-\alpha}}{Z}\\
  & = \log (1 - P(|x| > \alpha) + C_\alpha2^{-\alpha}/Z)\\
  & < \log (1 + |S|2^{L}2^{ - \alpha})\\
  & = \log (1 + 2^{ - \gamma})\\
  & < \frac{2^{-\gamma}}{\ln 2}\\
  \max_x |P(x) - Q(x)|& < \frac{2^{-\gamma/2}}{\sqrt{\ln 4}}\text{, Pinsker's inequality}
\end{align}
and so the kl divergence goes to zero exponentially quickly in $\gamma$, as does the total variation distance.
This suggests an alternative algorithm in which we never reject samples from $Q$ but instead specify a bound on the KL divergence which in turn gives us a bound on  $\gamma$.
This tightens the upper bound on expected run time given in the previous section, and makes the math simpler.

\subsection{Fluctuations around $Q$}
We don't sample exactly from $Q$ but from a related distribution $Q'$, because these XOR constraints introduce small fluctuations around the true distribution.
Let $x$ temporarily refer to an assignment both of the program and auxiliary variables,
and let $\mu = 2^{-K}(N - 1) + 1$ the average number of survivors given that $x$ survives:
\begin{align}
  p_1(x)& = 2^{-K}\sum_i \frac{1}{i}P(mc = i | x \text{ survives})\\
  & = 2^{-K} E[\frac{1}{mc} | x \text{ survives}]\\
  &  >2^{-K}  \frac{1}{E[mc | x \text{ survives}]}\text{, Jensen's inequality}\\
  & = 2^{-K} 1/\mu\\
  & = \frac{2^{-K}}{2^{-K}(N - 1) + 1}\\
  & = \frac{1}{N - 1 + 2^K}\\
  & = \frac{1}{N} \times \frac{1}{1 + 2^K/N -1/N}\\
  & > \frac{1}{N}\times  \frac{1}{1 + 2^K/N }\\
  D(Q||Q')& = \sum_x q(x)\log \frac{q(x)}{q'(x)}\\
  & = \sum_x q(x)\log \frac{|A_x|(1/N)}{\sum_{A_x}p_1(x,A_x)/P(mc > 0)}\\
  &  <  \log P(mc > 0) +  \sum_x q(x)\log \frac{|A_x|(1/N)}{\sum_{A_x}(1/N)/(1 + 2^K/N)}\\
  & = \log P(mc > 0) +  \sum_x q(x)\log \frac{|A_x|}{|A_x|/(1 + 2^K/N)}\\
  & = \log P(mc > 0) +  \sum_x q(x)\log (1 + 2^K/N)\\
  & = \log P(mc > 0) +  \log (1 + 2^K/N)\\
  & < 2^K/N\\
  & < 2^K/(|S| - 1 + 2^{\alpha - L})\\
  & = 2^{ - \delta}.
\end{align}



\subsubsection{Fluctuations around $P$}
Of course we actually care about $D(p||q_K)$, where $q_K$ is the distribution sampled by the algorithm.
The previous two sections were warm-ups for this calculation.
Let $x$ be a program.
Then $q_K(x) = p_1(x)A(|x|,\alpha)/W$ (where $W$ is the normalizing constant)
so:
\begin{align}
  D(p||q_K)& = \sum_x p(x)\log \frac{p(x)}{q_K(x)}\\
  & = \sum_x p(x)\log \frac{A(x)q(x)}{\sum_y A(y)q(y)}\frac{\sum_y A(y)q_k(y)}{A(x)q_k(x)}\text{, as }q(x)A(x)\propto p(x)\\
  & = \log \frac{\sum_y A(y)q_k(y)}{\sum_y A(y)q(y)}+\sum_x p(x)\log \frac{q(x)P(mc > 0)}{p_1(x)}\\
  &  <  \log \frac{\sum_y A(y)q_k(y)}{\sum_y A(y)q(y)} + \log \frac{1}{c} + \log P(mc > 0)\\
  & < \log \frac{\sum_{y\not= x_*} A(y)c'q(y)  +A(x_*)(1 - \sum_{y\not= x_*} c'q(y))}{\sum_y A(y)q(y)} + \log\frac{P(mc > 0)}{c} \\
  &  =  \log \frac{\sum_{y\not= x_*} A(y)c'q(y) +1 - \sum_{y\not= x_*} c'q(y)}{\sum_y A(y)q(y)} + \log\frac{P(mc > 0)}{c} \\
  &  =  \log \frac{1 + c'\sum_{y\not= x_*} q(y)(A(y)-1)}{\sum_y A(y)q(y)} + \log\frac{P(mc > 0)}{c} \\
  &  =  \log \frac{1 + c'\sum_{y} q(y)(A(y)-1)}{\sum_y A(y)q(y)} + \log\frac{P(mc > 0)}{c} \\
  &  =  \log \frac{1 + c'(\sum_{y} q(y)A(y) - 1)}{\sum_y A(y)q(y)} + \log\frac{P(mc > 0)}{c} \\
  &  =  \log \left( c' + \frac{1 - c'}{P(\text{accept})} \right) + \log\frac{P(mc > 0)}{c} \text{, where }c' = \frac{c}{P(mc>0)}\\
  & = \log  \left( 1 + \frac{1 - c'}{P(\text{accept})}\times \frac{P(mc > 0)}{c} \right)\\
  & = \log  \left( 1 + \frac{(1 + 2^K/N - 1/N)P(mc > 0) - 1}{P(\text{accept from $Q$})} \right) \label{closeBound}\\
  & < \log  \left( 1 + \frac{2^K/N}{P(\text{accept from $Q$})} \right) \\
  & < \log  \left( 1 + 2^{ - \delta}(1 + 2^{ - \gamma}) \right)  < 2^{ - \delta} + 2^{ - \delta - \gamma} < 2^{ - \delta + 1}
\end{align}
And so the KL divergence decreases exponentially in $\delta$, and can be closely bounded above after enumerating all of the programs by equation~\ref{closeBound}.
It is easier to show that a bound with the similar asymptotics (specifically, $O(2^{-\min (\delta, \gamma )}$) holds whenever we don't perform the rejection,
which is interesting,
and suggests that the rejection sampling isn't buying us very much.
So the amount of work we have to do grows exponentially as we move away from the diagonal,
and the KL divergence decays exponentially as we move away from the diagonal.
This suggests going as close to the diagonal as the bound on the KL will allow.

\subsection{Entropy in the program distribution}
\begin{align}
  H[P] &=  \sum_x 1/Z2^{-|x|} \log Z2^{|x|}\\
  & = \sum_x 1/Z 2^{-x} (\log Z + x)\\
  & = \log Z + \sum_x P(x)|x|\\
  & = \log Z + E_P[|x|]\\
\end{align}
Use the Markov inequality to relate this to $C_\alpha$:
\begin{align}
  P(|x| > l) & < E_p[|x|]/l = (H[P] - \log Z)/l %l \sum_{|x| > l}\frac{2^{|x|}}{Z}& < (H[P] - \log Z)/l
  \\  lP(|x| > l) + \log Z& < H[p]\\
  Z& < 2^{H[P] - lP(|x| > l)}
\end{align}
Let $l > 0$:
\begin{align}
  C_{  <  l}& = \sum_{|x| < l}1\\
  & = \sum 2^l2^{-l}\\
  &   < Z  2^l\sum_{|x| > l}P(x)\\
  & = Z2^lP(|x| > l)\\
  & < 2^H2^{lP(|x|<l)}P(|x|<l)\\
  & < 2^{H+l}
\end{align}
Which thus far seems to be useless, probably because the Markov inequality is super loose in practice.


\section{Experimental results}

\subsection{List manipulation}
This is a domain of recursive list manipulation programs designed to demonstrate the ability of program sampling to synthesize algorithms.
There are two goals here: (1) show that description length priors can give reasonable algorithm implementations from examples, and (2) show that by maintaining a full posterior over programs we get better predictive accuracy than just picking the most likely program.

Primitives include car, cdr, append, $\geq$, $\leq$, $0$, nil, increment, decrement, length, filter, and a list singleton constructor.
The sketch constrains the space by (1) specifying that there is exactly one base case guarded by one conditional, and (2) specifying that the return value is a concatenation of lists which may be optionally recursed upon.
This is expressive enough to encode sorting, reversing, indexing, counting, and searching through lists.

\subsubsection{Sorting}
Given a these few examples, can we learn a posterior over sorting programs? In particular, does sampling help us in this domain?
\begin{verbatim}
f([]) = [], f([5]) = [5], f([7,9]) = [7,9], f([2,7]) = [2,7], 
f([5,4,3]) = [3,4,5], f([5,3,4]) = [3,4,5]
\end{verbatim}

I sampled 10 programs from the posterior using $K = 13$, $\alpha -L = 11$ and noticed the following program within the posterior:
\begin{verbatim}
(if ((eq 0) (length a))
    nil
    (append (recur (filter (gt (car a)) (cdr a)))
            (list (car a))
            (recur (filter (lt (car a)) (cdr a)))))
\end{verbatim}
Which is a recursive implementation of quicksort. Interestingly this is not the most likely program, which only uses two conditional branches and effectively only recursives on the smaller list elements (intentionally omitted from the training set is an example that would force it to recursive in this case).
So if you do map inference you get a hard decision that fails for many lists,
but if you sample sorting algorithms and then marginalize over the program you get a soft decision that includes the correct sorted list.

Generating the samples took 40 minutes, compared to around a day to enumerate all 13000 solutions.
This is a factor of 1--2 orders of magnitude better, but much less than what you would expect by just looking at the number of solver invocations (50 for sampling, which is 2--3 orders of magnitude fewer than enumeration).
Empirically it seems that the first solver invocation takes a very long time,
and subsequent invocations take on the order of seconds.

I picked these values of $K$ and $\alpha$ by looking at Figure~\ref{sorting}, which plots upper bounds upon the run time and KL divergence.
\begin{figure}\label{sorting}
  \includegraphics[width = 10cm]{sort.png}
  \caption{Grayscale: log expected solver invocations. Colored contours: upper bound on the log KL divergence. }
\end{figure}

\subsection{Flashfill}
This is a domain of toy flash fill problems.

\subsubsection{Small solution set regime}
Problem:
\begin{verbatim}
Tom and Jerry      ---> Tom
Jack and Jill      ---> Jack
\end{verbatim}
There are 12 satisfying solutions to this problem within the program space.
Thus we are trying to beat enumeration, which would take 13 calls to the solver.
Figure~\ref{ambiguous2} shows a heat map of the upper bound upon the numeber of solver invocations as a function of $\alpha$ and $K$.
\begin{figure}\label{ambiguous2}
  \includegraphics[width = 10cm]{ambiguous2.png}
  \caption{Grayscale: log expected solver invocations. Colored contours: upper bound on the log KL divergence. }
\end{figure}
What if we sample at the beginning of the dark region ($\alpha = 27,K = 5$)? Below is the output of the sampler:
\begin{verbatim}
|s| = 1 	log_2(z) = -19.0 	1/p = 524288.0 	shortest = 19 bits
Implicitly enumerated 8 satisfying solutions
Samples:
SubString(0,pos([],["' '"],0)) 19
Length bounded by alpha, so accepted.
total time =  7.25420975685
\end{verbatim}
So we were able to get away with only 5 random constraints. What if we go into the lighter region at $\alpha = 26$,$K = 2$? We observe that there are more programs enumerated and that some of them are rejected:
\begin{verbatim}
|s| = 5 	log_2(z) = -18.955264207 	1/p = 508280.094574 	shortest = 19 bits
Implicitly enumerated 36 satisfying solutions
Samples:
SubString(0,pos([],["' '"],0)) 19
Length bounded by alpha, so accepted.
SubString(0,pos([],["' '"],0)) 19
Length bounded by alpha, so accepted.
SubString(0,pos([],["' '", "'U'"],0)) 24
Length bounded by alpha, so accepted.
SubString(0,pos([],["' '"],0)) 19
Length bounded by alpha, so accepted.
SubString(0,pos([],["' '"],0)) 19
Length bounded by alpha, so accepted.
SubString(0,1)++SubString(1,2)++SubString(2,pos([],["' '"],0)) 42
Rejected.
SubString(0,pos([],["' '"],0)) 19
Length bounded by alpha, so accepted.
SubString(0,pos([],["' '", "'U'"],0)) 24
Length bounded by alpha, so accepted.
SubString(0,pos([],["' '"],0)) 19
Length bounded by alpha, so accepted.
SubString(0,pos([],["' '"],0)) 19
Length bounded by alpha, so accepted.
total time =  7.45482897758
\end{verbatim}
Lastly, does this scheme for reducing the number of constraints you need help for this problem?
Put $\alpha = 52$ and $K = 31$.
\begin{verbatim}
|s| = 1 	log_2(z) = -19.0 	1/p = 524288.0 	shortest = 19 bits
Implicitly enumerated 8 satisfying solutions
Samples:
SubString(0,pos([],["' '"],0)) 19
Length bounded by alpha, so accepted.
SubString(0,pos([],["' '"],0)) 19
Length bounded by alpha, so accepted.
SubString(0,pos([],["' '"],0)) 19
Length bounded by alpha, so accepted.
SubString(0,pos([],["' '"],0)) 19
Length bounded by alpha, so accepted.
SubString(0,pos([],["' '"],0)) 19
Length bounded by alpha, so accepted.
SubString(0,pos([],["' '"],0)) 19
Length bounded by alpha, so accepted.
SubString(0,pos([],["' '"],0)) 19
Length bounded by alpha, so accepted.
SubString(0,pos([],["' '"],0)) 19
Length bounded by alpha, so accepted.
SubString(0,pos([],["' '"],0)) 19
Length bounded by alpha, so accepted.
SubString(0,pos([],["' '"],0)) 19
Length bounded by alpha, so accepted.
total time =  155.674251795
\end{verbatim}
So without picking a small value of $K$ we are actually slower than just enumerating all of the programs.
Empirically the solver time seems to be blowing up around $K = 31$,
thus we have good reason to not set $\alpha$ to $n$, which is about 150 bits for this program space.

\subsubsection{Large solution set regime}
If we remove the second training example, the correct program becomes highly ambiguous:
\begin{verbatim}
Tom and Jerry     -> Tom
\end{verbatim}
There are 55434 possible programs in the space, and once we apply the embedding this number grows exponentially in $\alpha$.
It takes about 2300 seconds to enumerate these programs, but we can sample them much more efficiently with $K=15$, $\alpha = 32$:
\begin{verbatim}
|s| = 7 	log_2(z) = -11.9887727446 	1/p = 4064.24806201 	shortest = 12 bits
Implicitly enumerated 38 satisfying solutions
Samples:
SubString(0,3) 12
Length bounded by alpha, so accepted.
SubString(0,3) 12
Length bounded by alpha, so accepted.
SubString(0,3) 12
Length bounded by alpha, so accepted.
SubString(0,3) 12
Length bounded by alpha, so accepted.
SubString(0,3) 12
Length bounded by alpha, so accepted.
Const["'U'", 25, 23] 19
Length bounded by alpha, so accepted.
SubString(0,3) 12
Length bounded by alpha, so accepted.
Const["'U'", 25, 23] 19
Length bounded by alpha, so accepted.
SubString(0,3) 12
Length bounded by alpha, so accepted.
SubString(0,3) 12
Length bounded by alpha, so accepted.
total time =  17.3803441525
\end{verbatim}
Notice that this is on a log scale due to the very large number of solutions.
\begin{figure}
  \includegraphics[width = 10cm]{ambiguous.png}
    \caption{Grayscale: log expected solver invocations. Colored contours: upper bound on the log KL divergence. }
\end{figure}

\subsubsection{High description length programs}
One benefit of using a solver is that it is sometimes easy to find programs even if they have a high description length,
which can be caused by having some long constant in them.
For example, consider the following training data:
\begin{verbatim}
Eyal Dechter     -> Hi Eyal!
Rishabh Singh    -> Hi Rishabh!
\end{verbatim}

With only the first example, the performance curve looks like:
\begin{figure}
  \includegraphics[width = 10cm]{long1.png}
  \caption{Grayscale: log expected solver invocations. Colored contours: upper bound on the log KL divergence. }
\end{figure}

With two examples, there is only one satisfying solution (the program is uniquely determined).
\begin{figure}
  \includegraphics[width = 10cm]{long2.png}  
\end{figure}


\end{document}
