\documentclass{article}
\usepackage{graphicx}
\usepackage{amsmath}

\begin{document}

\section{High-level overview}

We want to sample programs from a description length prior.
Let $x$ be a program. Sample from
\begin{equation}
  P(x) = \frac{2^{-|x|}}{Z}
\end{equation}
Assume $|x|\leq n$ always, where $n$ is the number of SAT variables encoding the structure of the program.

The approach will draw from two techniques: (1) constraint-solving techniques for program synthesis; and (2) uniform solution sampling using XOR constraints.

The two most obvious approaches to this problem don't work in practice:

\begin{itemize}
\item{Prefix codes}: Write down a prefix code for the space of allowed programs.
  Uniformly sample values of all $n$ SAT variables encoding the program structure;
  then the probability of sampling $x$ is proportional to $2^{n-|x|}$.
\item{Embed \& Project}: Introduce $n$ auxiliary variables, $A$,
  and add constraints $|x| > j\implies A_{j+1}$.
  Uniformly sample solutions to $(x,A)$.
  This is the technique introduced in ``Embed and Project'' (Stefano Ermon, NIPS 2013)
\end{itemize}

The problem with these approaches is that
they dramatically increase the number of satisfyable solutions to the underlying SAT formula by a factor of $O(2^n)$,
and so require at least about $n$ random constraints;
for large $n$, the solver scales exponentially with the number of random constraints (empirically observed for even CryptoMiniSAT).
The body of work on sampling with random constraints has previously restricted itself to cases where the number of satisfiable solutions is relatively small ($ < 600$ in the original Gomes et al. NIPS paper; on the order of tens of thousands in later work, by the authors of UniGen).

Our idea is to use auxiliary variables as in ``Embed and Project,'' but reduce the dimensionality of the embedding
by only hashing the first $\alpha$ bits of $A$.
If we put $\alpha = n$ then our samples come from $P$; otherwise they come from a different distribution $Q$:
\begin{equation}
  Q(x) \propto 2^{-\min (\alpha,|x| )}
  \end{equation}
We can correct for this proposal distribution by accepting a sample $x$ from $Q$ with probability $A(|x|,\alpha) = 2^{\min(0,\alpha - |x|)}$.
So we accept any sample whose description length is bounded by $\alpha$ and accept longer samples with probability $2^{\alpha - |x|}$.
This satisfies $Q(x)A(|x|,\alpha)\propto P(x)$.

The only reason that this works in the general case is because of the assumption that $|x|\leq n$,
which allows us to make $Q$ arbitrarily close to $P$ (and thus arbitrarily decrease the rejection rate),
but at the expense of needing more random constraints.
The rest of this document analyzes the trade-off between increasing $\alpha$ and increasing $K$ (the number of random constraints),
and then presents some experimental results.

\section{Algorithmic details}
\subsection{The algorithm}
Let $X$ be a set of programs satisfying some specification, such as agreeing with certain input-output pairs.
We want to sample from
\begin{equation}
  P(x\in X) = \frac{2^{-|x|}}{Z}
\end{equation}
where $Z = \sum_{x\in X}2^{-|x|}$.

Let $\alpha$ be the dimensionality of the embedding.
The set $E$ of programs in the embedded space is
\begin{equation}
  E = \{(x,A) : x\in X, |x|> j \implies A_{j+1}\text{ when } j<\alpha \}
\end{equation}

In each iteration of the algorithm we sample a random projection from $n+\alpha$ to $K$ variables, setting it equal to a random $K$ dimensional vector.
Let  $h(x,A)$ be true if $(x,A)\in E$ satisfies this random constraint.
Let $S = \{ e\in E : h(e)\}$.
If $S$ is empty, reject this attempt at sampling.
Otherwise draw uniformly from $S$ to obtain $(x,A)$,
and reject this attempt at sampling with probability $1 - A(|x|,\alpha) = 1 - 2^{\min(0,\alpha - |x|)}$.

The embedding introduces many symmetries (lots of $e$'s that have the same $x$),
and we can break this symmetry by only enumerating unique values of $x$.
For a given $x$, the number of $(x,A)\in S$ can be calculated without further calls to the solver:
it is $2^{\alpha - |x| - \text{rank}(H_{>n+|x|})}$ where $H$ is the matrix used for the random hash.

\subsection{Algorithm analysis}
First, what is the probability that a sample from $Q$ is accepted?
Call this $P(\text{accepted})$:
\begin{eqnarray}
  P(\text{accepted}) &=& \sum_x Q(x)A(|x|,\alpha)\\
  & = &\sum_x \frac{2^{-|x|}}{\sum_{x'}Q(x')}\\
  & = &\frac{Z}{\sum_{|x|>\alpha}2^{-\alpha} + \sum_{|x|\leq\alpha}2^{-|x|}}\\
  & = &\frac{Z}{Z - Z_\alpha + 2^{-\alpha}C_\alpha}\\
  & = &\frac{1}{1 + 2^{-\alpha}C_\alpha/Z - P(|x|>\alpha)}
\end{eqnarray}
where $C_\alpha = |\{x\in X : |x|>\alpha\}|$ and $Z_\alpha = \sum_{|x|>\alpha}2^{-|x|} $.

Second, what is the probability that we get a sample from $Q$?
Let $mc$ be the model count (members of $E$ that survived the hashing constraint).
Then $E[mc] = \mu = 2^{-K}|E|$ and $\text{Variance}[mc] = \sigma^2 = |E|2^{-K}(1 - 2^{-K})$.
We fail to get a sample if $mc<1$, equivalently, $mc\leq \epsilon$ for $0<\epsilon < \min (\mu,1)$.
\begin{eqnarray}
  P(mc\leq \epsilon)&<& P(|mc-\mu |\geq \mu -  \epsilon)\\
  &\leq&\frac{\sigma^2}{(\mu - \epsilon)^2}\text{, Chebyshev 's inequality}\\
  &\leq&\frac{\mu}{(\mu - \epsilon)^2}.
\end{eqnarray}

Put these together to bound the probability of failing to get a sample:
\begin{eqnarray}
  P(\text{fail to get sample})& = &P(mc = 0) + P(mc > 0)(1 - P(\text{accepted}))\\
  &<&\frac{\mu}{(\mu - \epsilon)^2} + 1 - \frac{1}{1 + 2^{-\alpha}C_\alpha/Z - P(|x|>\alpha)}\\
  P(\text{get sample})&>& \frac{1}{1 + 2^{-\alpha}C_\alpha/Z - P(|x|>\alpha)} - \frac{\mu}{(\mu - \epsilon)^2}\\
\end{eqnarray}

Now that we have a bound on the probability of getting a sample (and thus abound on the expected number of attempts needed to get a sample),
we want to analyze how much work we expect to do on each attempt at sampling.
A reasonable proxy for this is the number of invocations made to the solver,
which is just the number of programs in $E$ that survived the constraints (\emph{not} the number of surviving solutions).
Let $x$ be a program.
If $|x|>\alpha$ then the probability that it survives is just $2^{-K}$.
Otherwise it's more complicated.
Let $|x|\leq \alpha$, $H$ be the random matrix and $b$ the random vector.
The hashing constraint reads
\begin{equation}
  b_i = \sum_{j = 1}^n H_{ij}x_j + \sum_{j = n+1}^{n+|x|}H_{ij}A_{j-n} + \sum_{j = n + |x| + 1}^{n + \alpha}H_{ij}A_{j-n}
\end{equation}
The first two sums can be folded into $b_i$, giving
\begin{equation}
  b'_i = \sum_{j = n + |x| + 1}^{n + \alpha}H_{ij}A_{j-n}
\end{equation}
So $x$ survives if $b'\in\text{range}(H_{j\geq n + |x| + 1})$.
The matrix $H_{j\geq n + |x| + 1}$ is $K$ by $\alpha - |x|$; call it $G$.
\begin{eqnarray}
  P(x \text{ survives})& = &\sum_{r = 0}^{\min(K,\alpha - |x|)}P(b'\in\text{range}(G) | \text{rank}(G) = r)P(\text{rank}(G) = r)\\
  & = &\sum_r 2^{r - K}2^{-K(\alpha - |x|)}\prod_{0\leq i<r}\frac{(2^K - 2^i)(2^{\alpha - |x|} - 2^i)}{2^r - 2^i}\\
  &<&\min (1,2^{\alpha - |x| - K})
\end{eqnarray}
where the last inequality is not derived from the former inequality but from applying the union bound to the original statement.

What is the total expected work to get one sample?
Let $s_i(x)$ be an indicator variable for whether $x$ survives the $i$th sampling attempt,
and $f_i$ be an indicator variable for whether the $i$th sampling attempt failed.
the number of solver invocations is
\begin{eqnarray*}
  &&1 + \sum_x s_1(x)\\
  &+&f_1 + \sum_xs_2(x)f_1\\
  &+&f_1f_2 + \sum_xs_3(x)f_1f_2\\
  &+&\cdots
\end{eqnarray*}
which has the expectation
\begin{equation}
  \frac{1 + \sum_x P(x \text{ survives})}{P(\text{get sample})}
\end{equation}

We can upper bound this quantity using the previously bounded quantities.
The only approximation comes in here:
\begin{equation}
  p(\text{no survivors}) < \frac{\mu}{(\mu - \epsilon)^2}
\end{equation}
Thus by enumerating all of the solutions to a program synthesis problem we can compute fairly tight bounds on the performance of the proposed algorithm,
as the next section details.

In practice we want to sample without enumerating all of the solutions.
What does this bound tell us?

\begin{eqnarray}
  E[\text{invocations}]& =&   \frac{1 + \sum_x P(x \text{ survives})}{P(\text{get sample})}\\
  &<& \frac{1 + \sum_x P(x \text{ survives})}{\frac{1}{1 + 2^{-\alpha}C_\alpha/Z - P(|x|>\alpha)} - \frac{\mu}{(\mu - \epsilon)^2}}\\
  &<& \frac{1 + \sum_x P(x \text{ survives})}{\frac{1}{1 + 2^{-\alpha}C_\alpha/Z} - \frac{1}{\mu}}\\
  &<& \frac{1 + C_\alpha 2^{-K} + 2^{\alpha-K}\sum_{\alpha\geq |x|\geq \alpha-K}2^{-|x|} + \sum_{|x|<\alpha-K} 1}{\frac{1}{1 + 2^{-\alpha}C_\alpha/Z} - \frac{1}{\mu}}\\
  &<& \frac{1 + C_\alpha 2^{-K} + C_{\leq\alpha}}{\frac{1}{1 + 2^{-\alpha}C_\alpha/Z} - \frac{2^{K}}{|E|}}\\
  &<& \frac{|E|(1 + 2^{-\alpha}C_\alpha/Z)(1 + C_\alpha 2^{-K} + C_{\leq\alpha})}{|E| - 2^K(1 + 2^{-\alpha}C_\alpha/Z)}\\
\end{eqnarray}
We probably want $\alpha>K + |x_*|$, so that the best solution probably survives.
Also probably a bound is given on $K$
so introduce $\alpha - |x_*| = k+d$.

\begin{eqnarray}
  E[tt]& < &\frac{1 + C_\alpha 2^{-K} + 2^{\alpha-K}\sum_{\alpha\geq |x|\geq \alpha-K}2^{-|x|} + \sum_{|x|<\alpha-K} 1}{\frac{1}{1 + 2^{-\alpha}C_\alpha/Z} - \frac{1}{\mu}}\\
  & < &\frac{1 + C_\alpha 2^{-\alpha + |x_*| + d} + \sum_{\alpha\geq |x|}1}{\frac{1}{1 + 2^{-\alpha+|x_*|}C_\alpha} - \frac{1}{\mu}}\\
\end{eqnarray}


%We can lower bound $Z>2^{-|x_*|}$, 

\section{Experimental results}

\subsection{Flashfill}
This is a domain of toy flash fill problems.

\subsubsection{Small solution set regime}
Problem:
\begin{verbatim}
Tom and Jerry      ---> Tom
Jack and Jill      ---> Jack
\end{verbatim}
There are 12 satisfying solutions to this problem within the program space.
Thus we are trying to beat enumeration, which would take 13 calls to the solver.
Below is a heat map of the upper bound upon the numeber of solver invocations as a function of $\alpha$ and $K$:
\begin{figure}
  \includegraphics[width = 10cm]{heat.png}
\end{figure}
What if we sample at the beginning of the dark region ($\alpha = 27,K = 5$)? Below is the output of the sampler:
\begin{verbatim}
|s| = 1 	log_2(z) = -19.0 	1/p = 524288.0 	shortest = 19 bits
Implicitly enumerated 8 satisfying solutions
Samples:
SubString(0,pos([],["' '"],0)) 19
Length bounded by alpha, so accepted.
total time =  7.25420975685
\end{verbatim}
So we were able to get away with only 5 random constraints. What if we go into the lighter region at $\alpha = 26$,$K = 2$? We observe that there are more programs enumerated and that some of them are rejected:
\begin{verbatim}
|s| = 5 	log_2(z) = -18.955264207 	1/p = 508280.094574 	shortest = 19 bits
Implicitly enumerated 36 satisfying solutions
Samples:
SubString(0,pos([],["' '"],0)) 19
Length bounded by alpha, so accepted.
SubString(0,pos([],["' '"],0)) 19
Length bounded by alpha, so accepted.
SubString(0,pos([],["' '", "'U'"],0)) 24
Length bounded by alpha, so accepted.
SubString(0,pos([],["' '"],0)) 19
Length bounded by alpha, so accepted.
SubString(0,pos([],["' '"],0)) 19
Length bounded by alpha, so accepted.
SubString(0,1)++SubString(1,2)++SubString(2,pos([],["' '"],0)) 42
Rejected.
SubString(0,pos([],["' '"],0)) 19
Length bounded by alpha, so accepted.
SubString(0,pos([],["' '", "'U'"],0)) 24
Length bounded by alpha, so accepted.
SubString(0,pos([],["' '"],0)) 19
Length bounded by alpha, so accepted.
SubString(0,pos([],["' '"],0)) 19
Length bounded by alpha, so accepted.
total time =  7.45482897758
\end{verbatim}
Lastly, does this scheme for reducing the number of constraints you need help for this problem?
Put $\alpha = 52$ and $K = 31$.
\begin{verbatim}
|s| = 1 	log_2(z) = -19.0 	1/p = 524288.0 	shortest = 19 bits
Implicitly enumerated 8 satisfying solutions
Samples:
SubString(0,pos([],["' '"],0)) 19
Length bounded by alpha, so accepted.
SubString(0,pos([],["' '"],0)) 19
Length bounded by alpha, so accepted.
SubString(0,pos([],["' '"],0)) 19
Length bounded by alpha, so accepted.
SubString(0,pos([],["' '"],0)) 19
Length bounded by alpha, so accepted.
SubString(0,pos([],["' '"],0)) 19
Length bounded by alpha, so accepted.
SubString(0,pos([],["' '"],0)) 19
Length bounded by alpha, so accepted.
SubString(0,pos([],["' '"],0)) 19
Length bounded by alpha, so accepted.
SubString(0,pos([],["' '"],0)) 19
Length bounded by alpha, so accepted.
SubString(0,pos([],["' '"],0)) 19
Length bounded by alpha, so accepted.
SubString(0,pos([],["' '"],0)) 19
Length bounded by alpha, so accepted.
total time =  155.674251795
\end{verbatim}
So without picking a small value of $K$ we are actually slower than just enumerating all of the programs.
Empirically the solver time seems to be blowing up around $K = 31$,
thus we have good reason to not set $\alpha$ to $n$, which is about 150 bits for this program space.

\subsubsection{Large solution set regime}
If we remove the second training example, the correct program becomes highly ambiguous:
\begin{verbatim}
Tom and Jerry     -> Tom
\end{verbatim}
There are 55434 possible programs in the space, and once we apply the embedding this number grows exponentially in $\alpha$.
It takes about 2300 seconds to enumerate these programs, but we can sample them much more efficiently with $K=15$, $\alpha = 32$:
\begin{verbatim}
|s| = 7 	log_2(z) = -11.9887727446 	1/p = 4064.24806201 	shortest = 12 bits
Implicitly enumerated 38 satisfying solutions
Samples:
SubString(0,3) 12
Length bounded by alpha, so accepted.
SubString(0,3) 12
Length bounded by alpha, so accepted.
SubString(0,3) 12
Length bounded by alpha, so accepted.
SubString(0,3) 12
Length bounded by alpha, so accepted.
SubString(0,3) 12
Length bounded by alpha, so accepted.
Const["'U'", 25, 23] 19
Length bounded by alpha, so accepted.
SubString(0,3) 12
Length bounded by alpha, so accepted.
Const["'U'", 25, 23] 19
Length bounded by alpha, so accepted.
SubString(0,3) 12
Length bounded by alpha, so accepted.
SubString(0,3) 12
Length bounded by alpha, so accepted.
total time =  17.3803441525
\end{verbatim}
Notice that this is on a log scale due to the very large number of solutions.
\begin{figure}
  \includegraphics[width = 10cm]{ambiguous.png}  
\end{figure}

\subsubsection{High description length programs}
One benefit of using a solver is that it is sometimes easy to find programs even if they have a high description length,
which can be caused by having some long constant in them.
For example, consider the following training data:
\begin{verbatim}
Eyal Dechter     -> Hi Eyal!
Rishabh Singh    -> Hi Rishabh!
\end{verbatim}

With only the first example, the performance curve looks like:
\begin{figure}
  \includegraphics[width = 10cm]{long1.png}  
\end{figure}

With two examples, there is only one satisfying solution (the program is uniquely determined).
\begin{figure}
  \includegraphics[width = 10cm]{long2.png}  
\end{figure}


\end{document}
